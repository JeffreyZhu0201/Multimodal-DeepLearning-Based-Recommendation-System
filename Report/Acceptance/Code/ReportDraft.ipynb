{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f80a0f7",
   "metadata": {},
   "source": [
    "# Deep Learning Recommenddation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa87976",
   "metadata": {},
   "source": [
    "## 数据集准备\n",
    "基于Movie-Lens 32M数据集进行实验\n",
    "*ml-32m*\n",
    "* ml-32m/ratings.csv\n",
    "* ml-32m/movies.csv\n",
    "* ml-32m/tags.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83430a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ml-32m/ratings.csv\n",
    "userId,movieId,rating,timestamp\n",
    "1,17,4.0,944249077\n",
    "1,25,1.0,944250228\n",
    "1,29,2.0,943230976\n",
    "1,30,5.0,944249077\n",
    "# ml-32m/movies.csv\n",
    "movieId,title,genres\n",
    "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\n",
    "2,Jumanji (1995),Adventure|Children|Fantasy\n",
    "3,Grumpier Old Men (1995),Comedy|Romance\n",
    "4,Waiting to Exhale (1995),Comedy|Drama|Romance\n",
    "5,Father of the Bride Part II (1995),Comedy\n",
    "# ml-32m/tags.csv\n",
    "userId,movieId,tag,timestamp\n",
    "22,26479,Kevin Kline,1583038886\n",
    "22,79592,misogyny,1581476297\n",
    "22,247150,acrophobia,1622483469\n",
    "34,2174,music,1249808064\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccfbef",
   "metadata": {},
   "source": [
    "## 基础NCF模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e266b52",
   "metadata": {},
   "source": [
    "#### 导入必要包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271cb8c",
   "metadata": {},
   "source": [
    "#### 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "ratings = pd.read_csv('../Dataset/ml-32m/ratings.csv')\n",
    "csv_file_path = 'loss_data.csv'\n",
    "print(\"数据读取成功\")\n",
    "# 创建用户和电影映射字典,将稀疏数据稠密化\n",
    "user_ids = ratings['userId'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "movie_ids = ratings['movieId'].unique()\n",
    "movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为连续索引\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
    "# 归一化评分到0-1范围\n",
    "ratings['rating'] = ratings['rating'] / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf291b",
   "metadata": {},
   "source": [
    "#### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6429e",
   "metadata": {},
   "source": [
    "#### 实现Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Dataset类\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long),\n",
    "            torch.tensor(self.movies[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202d1bb",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "batch_size = 2048\n",
    "\n",
    "train_dataset = MovieLensDataset(train_df['user_idx'].values, \n",
    "                               train_df['movie_idx'].values,\n",
    "                               train_df['rating'].values)\n",
    "val_dataset = MovieLensDataset(val_df['user_idx'].values,\n",
    "                             val_df['movie_idx'].values,\n",
    "                             val_df['rating'].values)\n",
    "test_dataset = MovieLensDataset(test_df['user_idx'].values,\n",
    "                              test_df['movie_idx'].values,\n",
    "                              test_df['rating'].values)\n",
    "print(train_dataset[0])  # 打印第一个样本以验证数据集\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,pin_memory=True)\n",
    "print(\"数据加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c1ea6",
   "metadata": {},
   "source": [
    "#### 定义推荐模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义推荐模型\n",
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_emb = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user, movie):\n",
    "        user_emb = self.user_emb(user)\n",
    "        movie_emb = self.movie_emb(movie)\n",
    "        x = torch.cat([user_emb, movie_emb], dim=1)\n",
    "        return self.fc(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54451cfa",
   "metadata": {},
   "source": [
    "#### 加载模型，指定优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 初始化模型和优化器\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if(torch.cuda.is_available()):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"使用GPU加速\")\n",
    "print(device)\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "model = Recommender(n_users, n_movies).to(device)\n",
    "if os.path.exists('best_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    print(\"已加载保存的模型参数\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "loss_array = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} Training'):\n",
    "        user, movie, rating = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(user, movie)\n",
    "        loss = criterion(pred, rating)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * user.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1} Validation'):\n",
    "            user, movie, rating = [x.to(device) for x in batch]\n",
    "            pred = model(user, movie)\n",
    "            val_loss += criterion(pred, rating).item() * user.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    loss_array.append([train_loss,val_loss])\n",
    "\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # 保存损失数据到CSV文件\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['train_loss', 'val_loss'])  # 添加表头\n",
    "        csv_writer.writerows(loss_array)\n",
    "\n",
    "print(\"训练完成，开始测试阶段\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582e321",
   "metadata": {},
   "source": [
    "#### 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        user, movie, rating = [x.to(device) for x in batch]\n",
    "        pred = model(user, movie)\n",
    "        test_loss += criterion(pred, rating).item() * user.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "test_loss_csv = 'test_loss.csv'\n",
    "\n",
    "with open(test_loss_csv, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['test_loss'])  # 添加表头\n",
    "    csv_writer.writerow([test_loss])  # 写入测试损失\n",
    "\n",
    "print(loss_array)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test RMSE: {np.sqrt(test_loss * 5.0**2):.4f}')  # 反归一化后计算RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf08444",
   "metadata": {},
   "source": [
    "#### 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f23946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import  csv\n",
    "# 直接使用当前工作目录\n",
    "csv_path = 'loss_data.csv'\n",
    "\n",
    "# Read train_loss and test_loss from loss_data.csv\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "with open(csv_path, 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        train_loss.append(float(row['train_loss']))\n",
    "        val_loss.append(float(row['val_loss']))\n",
    "\n",
    "# Create epochs array\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8,6))\n",
    "# plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in train_loss],'p-',label=\"Train RMSE\")\n",
    "\n",
    "# plt.plot(epochs, test_loss, 'r-',label='val Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in val_loss],'p-',label=\"Validating RMSE\")\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Training and Validating Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "plt.ylabel(ylabel='RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e02a06",
   "metadata": {},
   "source": [
    "## 添加BERT模块对标题进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057edbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dbb57",
   "metadata": {},
   "source": [
    "#### 添加BERT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8530d407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000225FA028EF0>, 'Connection to huggingface.co timed out. (connect timeout=None)'))\"), '(Request ID: f94bdfe5-5e36-4170-8199-c382bd4ad5be)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    200\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    202\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    203\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connection.py:208\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    211\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x00000225FA028EF0>, 'Connection to huggingface.co timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000225FA028EF0>, 'Connection to huggingface.co timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 加载BERT tokenizer和模型\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m bert_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1968\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1964\u001b[0m                         vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1965\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1966\u001b[0m                         )\n\u001b[0;32m   1967\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1968\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m list_repo_templates(\n\u001b[0;32m   1969\u001b[0m                     pretrained_model_name_or_path,\n\u001b[0;32m   1970\u001b[0m                     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1971\u001b[0m                     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1972\u001b[0m                     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1973\u001b[0m                 ):\n\u001b[0;32m   1974\u001b[0m                     vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;66;03m# Get files from url, cache, or disk depending on the case\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py:163\u001b[0m, in \u001b[0;36mlist_repo_templates\u001b[1;34m(repo_id, local_files_only, revision, cache_dir)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    162\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 163\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[0;32m    164\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id, revision\u001b[38;5;241m=\u001b[39mrevision, path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    165\u001b[0m             )\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m         ]\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3140\u001b[0m, in \u001b[0;36mHfApi.list_repo_tree\u001b[1;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[0;32m   3138\u001b[0m encoded_path_in_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m quote(path_in_repo, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3139\u001b[0m tree_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_info \u001b[38;5;129;01min\u001b[39;00m paginate(path\u001b[38;5;241m=\u001b[39mtree_url, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursive\u001b[39m\u001b[38;5;124m\"\u001b[39m: recursive, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m: expand}):\n\u001b[0;32m   3141\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (RepoFile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_info) \u001b[38;5;28;01mif\u001b[39;00m path_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m RepoFolder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_info))\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_pagination.py:36\u001b[0m, in \u001b[0;36mpaginate\u001b[1;34m(path, params, headers)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m session \u001b[38;5;241m=\u001b[39m get_session()\n\u001b[1;32m---> 36\u001b[0m r \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(path, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     37\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\requests\\adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000225FA028EF0>, 'Connection to huggingface.co timed out. (connect timeout=None)'))\"), '(Request ID: f94bdfe5-5e36-4170-8199-c382bd4ad5be)')"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "# 加载BERT tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "bert_model.eval()  # 推理模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c5e8f",
   "metadata": {},
   "source": [
    "#### 定义电影名称转BERT向量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def title_to_bert_vec(title):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(title, return_tensors='pt', truncation=True, max_length=32).to(device)\n",
    "        outputs = bert_model(**inputs)\n",
    "        # 取[CLS]向量作为整体表示\n",
    "        cls_vec = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd58252",
   "metadata": {},
   "source": [
    "#### 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38589ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "ratings = pd.read_csv('../Dataset/ml-32m/ratings.csv')\n",
    "csv_file_path = 'loss_data.csv'\n",
    "print(\"数据读取成功\")\n",
    "# 创建用户和电影映射字典,将稀疏数据稠密化\n",
    "user_ids = ratings['userId'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "movie_ids = ratings['movieId'].unique()\n",
    "movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6315ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为连续索引\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
    "# 归一化评分到0-1范围\n",
    "ratings['rating'] = ratings['rating'] / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed409a1e",
   "metadata": {},
   "source": [
    "#### 生成BERT向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../Dataset/ml-32m/movies.csv')\n",
    "movieid2bertvec = {\n",
    "    row['movieId']: title_to_bert_vec(row['title'])\n",
    "    for _, row in movies.iterrows()\n",
    "}\n",
    "bert_dim = next(iter(movieid2bertvec.values())).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15892290",
   "metadata": {},
   "source": [
    "#### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19990504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7913138",
   "metadata": {},
   "source": [
    "#### 修改Dataset，返回BERT向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings, movie_ids, movieid2bertvec):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        self.movie_ids = movie_ids # movie_ids = movie_to_idx\n",
    "        self.movieid2bertvec = movieid2bertvec\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        movie_id = self.movie_ids[idx]\n",
    "        bert_vec = self.movieid2bertvec[movie_id]\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.movies[idx], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float).to(device),\n",
    "            torch.tensor(bert_vec, dtype=torch.float).to(device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2b8dc",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c896bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "batch_size = 2048\n",
    "\n",
    "train_dataset = MovieLensDataset(train_df['user_idx'].values, \n",
    "                               train_df['movie_idx'].values,\n",
    "                               train_df['rating'].values,\n",
    "                               train_df[\"movieId\"].values,\n",
    "                               movieid2bertvec)\n",
    "val_dataset = MovieLensDataset(val_df['user_idx'].values,\n",
    "                             val_df['movie_idx'].values,\n",
    "                             val_df['rating'].values,\n",
    "                             train_df[\"movieId\"].values,\n",
    "                             movieid2bertvec)\n",
    "test_dataset = MovieLensDataset(test_df['user_idx'].values,\n",
    "                              test_df['movie_idx'].values,\n",
    "                              test_df['rating'].values,\n",
    "                              train_df[\"movieId\"].values,\n",
    "                              movieid2bertvec)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,pin_memory=True)\n",
    "print(\"数据加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855fe90",
   "metadata": {},
   "source": [
    "#### 定义推荐模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64, hidden_dim=128, bert_dim=768):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_emb = nn.Embedding(num_movies, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            # 两个emb宽度加上bert_vec的宽度\n",
    "            nn.Linear(2*embedding_dim + bert_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//4, 1),\n",
    "            \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user, movie, bert_vec):\n",
    "        user_emb = self.user_emb(user)\n",
    "        movie_emb = self.movie_emb(movie)\n",
    "        x = torch.cat([user_emb, movie_emb, bert_vec], dim=1)\n",
    "        return self.fc(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b59a6",
   "metadata": {},
   "source": [
    "#### 加载模型，指定优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 初始化模型和优化器\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "model = Recommender(n_users, n_movies).to(device)\n",
    "if os.path.exists('best_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    print(\"已加载保存的模型参数\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44593ec0",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd943f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "loss_array = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} Training'):\n",
    "        user, movie, rating, bert_vec = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(user, movie, bert_vec)\n",
    "        loss = criterion(pred, rating)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * user.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1} Validation'):\n",
    "            user, movie, rating = [x.to(device) for x in batch]\n",
    "            pred = model(user, movie)\n",
    "            val_loss += criterion(pred, rating).item() * user.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    loss_array.append([train_loss,val_loss])\n",
    "\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # 保存损失数据到CSV文件\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['train_loss', 'val_loss'])  # 添加表头\n",
    "        csv_writer.writerows(loss_array)\n",
    "\n",
    "print(\"训练完成，开始测试阶段\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48596a04",
   "metadata": {},
   "source": [
    "#### 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        user, movie, rating, bert_vec = [x.to(device) for x in batch]\n",
    "        pred = model(user, movie, bert_vec)\n",
    "        test_loss += criterion(pred, rating).item() * user.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "test_loss_csv = 'test_loss.csv'\n",
    "\n",
    "with open(test_loss_csv, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['test_loss'])  # 添加表头\n",
    "    csv_writer.writerow([test_loss])  # 写入测试损失\n",
    "\n",
    "print(loss_array)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test RMSE: {np.sqrt(test_loss * 5.0**2):.4f}')  # 反归一化后计算RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6044f5",
   "metadata": {},
   "source": [
    "#### 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b705af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import  csv\n",
    "import os\n",
    "# 直接使用当前工作目录\n",
    "csv_path = 'loss_data.csv'\n",
    "\n",
    "# Read train_loss and test_loss from loss_data.csv\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "with open(csv_path, 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        train_loss.append(float(row['train_loss']))\n",
    "        val_loss.append(float(row['val_loss']))\n",
    "\n",
    "# Create epochs array\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8,6))\n",
    "# plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in train_loss],'p-',label=\"Train RMSE\")\n",
    "\n",
    "# plt.plot(epochs, test_loss, 'r-',label='val Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in val_loss],'p-',label=\"Validating RMSE\")\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Training and Validating Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "plt.ylabel(ylabel='RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
